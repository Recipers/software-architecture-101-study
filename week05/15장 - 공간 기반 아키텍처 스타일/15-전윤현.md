- 웹 기반 비즈니스 애플리케이션은 일반적으로 브라우저 -> 웹 서버 -> 애플리케이션 서버 -> 데이터베이스 서버 순서로 도달하는 요청 흐름을 가지고 있다.
  - 유저 수가 늘어나면 점점 병목 현상이 나타난다. 웹 서버 레이어, 애플리케이션 서버, 데이터베이스 레이어 순으로 병목이 발생한다.
  - 웹 서버를 확장한다고 해도, 확장하기 더 어려운 애플리케이션 서버에 병목이 생기고, 애플리케이션 서버를 확장해도 확장하기 더 어려운 데이터베이스 레이어에 병목이 생긴다.

<img width="536" alt="스크린샷 2025-06-10 오후 2 22 01" src="https://github.com/user-attachments/assets/d128166f-8273-4a17-9183-30d342412359" />

- 동시 유저 부하가 많은 대용량 애플리케이션은 데이터베이스의 동시 처리 가능한 트랜잭션 수 가 최종 제약조건이 되는 경우가 많다.
  - 다양한 캐시 기술과 데이터베이스 확장 제품으로 문제를 해결할 수 있지만, 부하가 극심할 때 애플리케이션을 확장하는 작업은 쉽지 않다.
- 공간 기반 아키텍처 스타일은 높은 확장성, 탄력성, 동시성 및 이와 관련된 문제를 해결하기 위해 설계된 아키텍처 스타일이다.
  - 동시 유저 수가 매우 가변적이라서 예측조차 곤란한 애플리케이션에 유용하다.
  - 극단적이고 가변적인 확장성 문제는 데이터베이스를 확장하거나 확장성이 떨어지는 아키텍처에 맞게 캐시 기술을 적용하는 것 보다 아키텍처적으로 해결하는 것이 더 낫다.

# 15.1 토폴로지
- 공간 기반 아키텍처라는 명층은 튜플 공간에서 유래됐다.
  - 튜플 공간은 공유 메모리를 통해 통신하는 다중 병렬 프로세서를 사용하는 기술이다.
  - 동기 제약조건인 중앙 데이터베이스를 없애는 대신, 복제된 인메모리 데이터 그리드를 활용하면 확장성, 탄력성, 성능을 높일 수 있다.
  - 애플리케이션 데이터는 메모리에 둔 상태로 모든 활성 처리 장치들이 데이터를 복제한다.
  - 처리 장치는 데이터를 업데이트할 때 퍼시스턴스 큐에 메시지를 보내는 식으로 데이터베이스에 데이터를 비동기 전송한다.
  - 유저 부하의 증가/감소에 따라 처리 장치는 동적으로 시작/종료할 수 있어 가변적으로 확장할 수 있다.
  - 중앙 데이터베이스가 애플리케이션의 표준 트랜잭션 처리에 관여하지 않으므로 데이터베이스 병목 현상이 사라진다.

<img width="572" alt="스크린샷 2025-06-10 오후 2 26 41" src="https://github.com/user-attachments/assets/b1e82eff-1058-436a-bf2e-bd50c45f72e2" />

- 공간 기반 아키텍처의 컴포넌트
  - 애플리케이션 코드가 구현된 처리 장치
  - 처리 장치를 관리/조정하는 가상 미들웨어
  - 업데이트된 데이터를 데이터베이스에 비동기 전송하는 데이터 펌프
  - 데이터 펌프에서 데이터를 받아 업데이트를 수행하는 데이터 라이터
  - 처리 장치가 시작되자마자 데이터베이스의 데이터를 읽어 전달하는 데이터 리더

### 15.1.1 처리 장치

<img width="345" alt="스크린샷 2025-06-10 오후 2 36 58" src="https://github.com/user-attachments/assets/0fd30cc7-0a81-41d8-87d1-d37cb94bf09c" />

- 애플리케이션 로직을 갖고 있다. 보통 웹 기반 컴포넌트와 백엔드 비즈니스 로직이 포함된다.
  - 작은 웹 기반 애플리케이션은 단일 처리 장치에 배포할 수 있지만, 대규모 애플리케이션은 기능별로 여러 장치에 나누어 배포해야 된다.
  - 애플리케이션 로직 외에도 Hazelcast, Apache Ignite 등의 제품에 있는 인메모리 데이터 그리드 및 복제 엔진도 포함된다
 
### 15.1.2 가상 미들웨어
- 아키텍처 내부에서 데이터 동기화 및 요청 처리의 다양한 부분을 제어하는 인프라를 담당한다.
  - 메시징 그리드, 데이터 그리드, 처리 그리드, 배포 관리자 등의 컴포넌트로 구성된다. 이들 컴포넌트들은 직접 작성하거나, 제품으로 구매할 수 있다.

#### 메시징 그리드

<img width="604" alt="스크린샷 2025-06-10 오후 2 39 17" src="https://github.com/user-attachments/assets/9fb189b1-6968-4092-9f51-72e3ccf99838" />

- 입력 요청과 세션 상태를 관리한다.
  - 가상 미들웨어에 요청이 유입되면 메시징 그리드는 어느 활성 처리 장치가 요청을 받아 처리할지 결정한다.
  - 단순한 라운드 로빈 알고리즘부터 처리 장치의 요청 처리 상태를 추적하는 복잡한 알고리즘까지 다양하다
  - 보통 부하 분산이 가능한 일반 웹 서버 (예: nginx)로 구현된다
 

#### 데이터 그리드

<img width="591" alt="스크린샷 2025-06-10 오후 2 41 38" src="https://github.com/user-attachments/assets/68096475-1a84-449f-837d-ba4da3a5f597" />

- 가장 중요하고 필수적인 컴포넌트이다.
  - 거의 대부분 복제 캐시로서 처리 장치에만 구현되어 있지만, 외부 컨트롤러가 필요한 복제 캐시 구현체나 분산 캐시를 사용할 경우 데이터 그리드는 가상 미들웨어 내부의 데이터 그리드 컴포넌트와 처리 장치 모두에 위치한다.
  - 메시징 그리드는 가용한 모든 처리 장치에 요청을 전달할 수 있으므로 각 처리 장치는 자신의 인메모리 데이터 그리드에 정확히 동일한 데이터를 갖고 있어야 한다.
  - 그림[15-5]는 처리 장치 간 동기식 데이터 복제 흐름을 나타낸 것이다. 실제로 동기화는 비동기 방식으로 매우 신속하게, 일반적으로 100밀리초 미만으로 이루어진다.

```java
HazelcastInstance hz = Hazlecast.newHazelcastInstance();
Map<String, CustomerProfile> profileCache = hz.getReplicatedMap("CustomerProfile");
```

- 데이터는 이름이 동일한 데이터 그리드가 포함된 처리 장치 간에 동기화된다.
  - 위의 코드는 고객 프로필 정보에 액세스하는 모든 처리 장치에 포함된다.
  - 만약 처리 장치 어디서건 CustomerProfile라는 캐시에 변경이 일어나면 동일한 이름의 캐시가 포함된 다른 모든 처리 장치에 변경된 데이터가 복제된다.
  - 처리 장치는 작업을 마치는데 필요한 만큼의 복제 캐시를 소유할 수 있다. 한 처리 장치가 다른 처리 장치를 원격 호출해서 데이터를 요청하거나(코레오그래피), 처리 그리드를 이용해서 요청을 오케스트레이트하는 방법도 있다.
- 데이터는 처리 장치 내부에서 복제되므로 데이터베이스에서 데이터를 읽지 않아도 서비스 인스턴스의 기동/중지가 가능하다
  - 단, 이름을 부여한 복제 캐시를 가진 인스턴스가 하나 이상 필요하다.
  - 처리 장치 인스턴스가 기동되면 캐시 프로바이더에 연결해서 이 캐시를 가져오라고 요청한다.
  - 다른 처리 장치에 접속되면 다른 인스턴스 중 한곳에서 캐시를 로드한다.
- 각 처리 장치는 멤버 리스트를 사용해 다른 모든 처리 장치의 인스턴스를 인지한다.
  - 멤버 리스트에는 동일한 이름의 캐시를 사용하는 다른 모든 처리 장치의 IP 주소 및 포트가 들어있다.
  - 멤버 리스트에 고객 프로필에 액세스하는 인스턴스가 인스턴스 1, 인스턴스 2, 인스턴스 3이 있을 때
    - 인스턴스 1이 고객 프로필 정보의 업데이트 요청을 받았을 때, 인스턴스 1은 cache.put()과 같은 업데이트 메서드로 캐시를 업데이트한다.
    - 데이터 그리드(예: 헤이즐캐스트, Apache Ignite)는 다른 복제 캐시도 똑같이 비동기 업데이트하는 식으로 세 개의 고객 프로필 캐시를 항상 똑같이 맞춘다.
    - 처리 장치 인스턴스가 다운되면 이 사실을 알 수 있게 다른 모든 처리 장치의 멤버 리스트가 자동으로 업데이트 된다.

#### 처리 그리드

<img width="591" alt="스크린샷 2025-06-10 오후 3 10 25" src="https://github.com/user-attachments/assets/0fb73d5c-acee-410b-95fc-4246fa97bec6" />

- 필수 컴포넌트는 아니지만, 다수의 처리 장치가 단일 비즈니스 요청을 처리할 경우 요청 처리를 오케스트레이트하는 일을 한다.
  - 또 종류가 다른 처리 장치 (예: 주문 처리 장치와 결제 처리 장치) 사이에 조정이 필요한 요청이 들어오면 두 처리 장치 사이에서 요청을 중재/조정한다.

#### 배포 관리자
- 부하 조건에 따라 처리 장치 인스턴스를 동적으로 시작/종료하는 컴포넌트이다.
  - 응답 시간, 유저 부하를 계속 모니터링하다 부하가 증가하면 새로운 처리 장치를 기동하고 반대로 감소하면 기존 처리 장치를 종료한다.

### 15.1.3 데이터 펌프

- 데이터를 다른 프로세서에 보내 데이터베이스를 업데이트하는 장치이다.
  - 처리 장치가 데이터를 데이트베이스에서 직접 읽고 쓰지 않으므로 데이터 펌프는 반드시 필요하다.
  - 데이터 펌프는 항상 비동기로 동작하면서 메모리 캐시와 데이터베이스의 최종 일관성을 실현한다.

<img width="352" alt="스크린샷 2025-06-10 오후 3 29 35" src="https://github.com/user-attachments/assets/a3660d43-f2f3-42eb-bfaa-ede84b048cc6" />

- 대개 메시징 기법으로 구현된다. 공간 기반 아키텍처에서 메시징은 데이터 펌프를 구현하는 효과적인 방법이다.
  - 메시징은 비동기 통신을 지원하고, 전달을 보장하며, FIFO 큐를 통해 메시지 순서를 유지한다.
  - 메시징을 이용하면 처리 장치와 데이터 라이터를 분리할 수 있기 때문에 데이터 라이터를 사용할 수 없을 경우에도 처리 장치에서 무중단 처리가 가능하다.
- 대부분의 경우 데이터 펌프는 도메인이나 그 서브도메인 별로 여러 개를 사용한다.
  - 캐시 종류(예: CustomerProfile, CustomerWishlist 등) 별로 전용 데이터 펌프를 두거나, 이 보다 훨씬 더 큰 일반적인 캐시를 포함한 처리 장치 도메인(예: Customer)별로 배정할 수도 있다.
  - 데이터 펌프는 계약 데이터와 연관된 액션(추가, 삭제, 수정)을 포함한다. 계약 포맷은 JSON 스키마, XML 스키마, 객체, 값 기반 메시지 등 다양하다.
  - 업데이트 데이터는 보통 데이터 펌프 안에 새 데이터 값만 보관한다.

### 15.1.4 데이터 라이터

<img width="577" alt="스크린샷 2025-06-10 오후 3 37 52" src="https://github.com/user-attachments/assets/d0f60b26-550f-4b2f-a2ee-03f6af00d50c" />

- 데이터 펌프에서 메시지를 받아 그에 맞게 데이터베이스를 업데이트하는 컴포넌트이다.
  - 서비스나 애플리케이션 데이터 허브(예" Ab Initio)로 구현할 수 있다.
  - 데이터 라이터의 세분도는 데이터 펌프와 처리 장치의 범위마다 다르다.
- 도메인 기반의 데이터 라이터는 데이터 펌프 수와 무관하게 특정 도메인(예: 고객)의 전체 업데이트를 처리하는데 필요한 모든 데이터베이스 로직을 가지고 있다.

<img width="629" alt="스크린샷 2025-06-10 오후 3 43 21" src="https://github.com/user-attachments/assets/5d8638b3-fc89-48c8-b19d-fad288f930f8" />

- 처리 장치 클래스마다 자체 전용 데이터 라이터를 두는 경우도 있다.
  - 각 데이터 라이터가 자신의 전용 데이터 펌프를 소유하고 해당 처리 장치(예: 위시리스트)의 데이터베이스 로직을 처리한다.
  - 데이터 라이터가 너무 많은 단점이 있지만, 처리 장치, 데이터 펌프, 데이터 라이터가 나란히 정렬되어 확장성 민첩성은 더 좋다.

### 15.1.5 데이터 리더
- 데이터베이스에서 데이터를 읽어 리버스 데이터 펌프를 통해 처리 장치로 실어 나르는 컴포넌트이다.
  - 공간 기반 아키텍처에서 데이터 리더는 세 가지 경우에만 작동된다.
    - 동일한 이름의 캐시를 가진 모든 처리 장치 인스턴스가 실패하는 경우
    - 동일한 이름의 캐시 안에서 모든 처리 장치를 재배포하는 경우
    - 복제 캐시에 들어있지 않은 아카이브 데이터를 조회하는 경우

<img width="629" alt="스크린샷 2025-06-10 오후 4 57 22" src="https://github.com/user-attachments/assets/ed811e4e-1d9e-4a9f-b169-dd63bdda2b26" />

- 인스턴스가 모조리 다운되면 데이터는 데이터베이스에서 읽어올 수 밖에 없다.
  - 처리 장치 인스턴스가 하나 둘 살아나기 시작하면서 각 인스턴스는 캐시에서 락을 획득하려고 한다.
  - 락을 손에 넣은 첫 번째 인스턴스는 임시 캐시 소유자가 된다.
  - 임시 캐시 소유자가 된 인스턴스는 데이터를 요청하는 큐에 메시지를 보내 캐시를 로드한다.
  - 그러면 데이터 리더가 읽기 요청을 받아 데이터베이스를 쿼리하여 처리 장치에 필요한 데이터를 검색하고 그 결과 데이터를 리버스 데이터 펌프로 보낸다
  - 임시 캐시 소유자인 처리 장치는 리버스 데이터 펌프에서 데이터를 받아 캐시를 로드한다.
  - 이 작업이 모두 끝나면 임시 소유자는 캐시 락을 해제하고 다른 모든 인스턴스가 동기화되면 처리를 개시한다
- 데이터 리더도 데이터 라이터처럼 도메인 기반으로 할 수 있지만 특정 처리 장치의 클래스 전용으로 사용하는게 보통이다.
  - 서비스, 애플리케이션, 데이터 허브 모두 구현체는 데이터 라이터와 동일하다
- 데이터 라이터와 데이터 리더는 본질적으로 데이터 추상 레이어(어떤 경우에는 데이터 액세스 레이어)를 형성한다.
  - 두 레이어의 차이점은 처리 장치가 데이터베이스의 테이블(또는 스키마) 구조를 얼마나 자세히 알고 있는가 이다.
    - 데이터 액세스 레이어는 처리 장치가 데이터베이스의 하부 데이터 구조와 커플링되어 있으므로 데이터 리더/라이터만 사용해서 데이터베이스에 액세스한다.
    - 데이터 추상 레이어는 처리 장치가 별도의 계약에 의해 하부 데이터베이스의 테이블 구조와 분리되어 있다.
  - 일반적으로 공간 기반 아키텍처는 데이터 추상 레이어 모델에 기반하므로 처리 장치마다 복제 캐시 스키마는 하부 데이터베이스의 테이블 구조와 다를 수 있다.
  - 따라서 처리 장치에 영향을 미치지 않고서도 데이터베이스 증분 변경이 가능하다. 데이터 리더/라이터에 이미 변환 로직이 포함되어 있기 때문에 이런 변경이 더 용이하다

# 15.2 데이터 충돌
- 이름이 동일한 캐시가 포함된 서비스 인스턴스에서 시시각각 업데이트가 일어나는 active/active 상태에서 복제 캐시를 사용하면 데이터 충돌이 발생할 수 있다.
  - 한 캐시 인스턴스(캐시 A)에서 데이터가 업데이트되어 다른 캐시 인스턴스(캐시 B)에 복제하는 도중에 동일한 데이터가 해당 캐시(캐시B)에서 업데이트되는 현상을 말한다.
  - 결국 캐시 B의 로컬 업데이트는 캐시 A에서 복제된 옛 데이터 때문에 덮어씌워지고, 반대로 캐시 A에서는 동일한 데이터가 캐시 B에서 발생한 업데이트 때문에 덮어씌워지는 불상사가 일어난다.
- 데이터 충돌 발생 빈도는 동일한 캐시를 포함한 처리 장치 인스턴스 수, 캐시 업데이트율, 캐시 크기, 캐시 제품의 복제 레이턴시 등의 여러 팩터가 영향을 미친다.
  - 정리된 수학 공식을 이용하여 데이터 충돌률을 근사하여 복제 캐시가 쓸 만한지 미리 파악하는 것이 유용하다.
 
# 15.3 클라우드 대 온프레미스 구현
- 공간 기반 아키텍처는 배포 환경 측면에서 독자적인 선택지가 있다.
  - 처리 장치, 가상 미들웨어, 데이터 펌프, 데이터 리더/라이터, 데이터베이스 등 전체 토폴로지는 클라우드 기반이나 온프레미스에 배포할 수 있다
  - 하지만 하이브리드한 환경에도 배포할 수 있는데, 이것이 다른 아키텍처 스타일에서는 찾아볼 수 없는 이 아키텍처의 특징이다.
 
<img width="620" alt="스크린샷 2025-06-10 오후 5 18 21" src="https://github.com/user-attachments/assets/d906c306-9a56-4295-8cf1-ce57f58dd2aa" />

- 물리 데이터베이스와 데이터를 온프레미스에 그대로 둔 상태로, 클라우드 기반의 매니지드 환경에서 처리 장치와 가상 미들웨어를 통해 애플리케이션을 배포하는 것이 가능한 것이 공간 기반 아키텍처의 강점이다.
- 이러한 토폴로지는 비동기 데이터 펌프와 이 아키텍처 스타일의 최종 일관성 모델 덕분에 아주 효율적인 클라우드 기반의 데이터 동기화가 가능하다.

# 15.4 복제 캐시 대 분산 캐시

- 공간 기반 아키텍처는 캐시 기술을 활용하여 애플리케이션 트랜잭션을 처리하고 데이터베이스에 직접 읽기/쓰기를 할 필요가 없어 확장성, 탄력성, 성능이 우수하다.
  - 대부분의 공간 기반 아키텍처는 복제 캐시를 사용하지만 분산 캐시도 사용할 수 있다.
 
<img width="585" alt="스크린샷 2025-06-10 오후 5 21 34" src="https://github.com/user-attachments/assets/6518d0cc-f0d2-48b1-a7a9-385a43509d2c" />

- 복제 캐시를 사용할 경우 각 처리 장치는 이름이 동일한 캐시를 사용하는 모든 처리 장치 간에 동기화되는 자체 인메모리 데이터 그리드를 갖고 있다.
  - 한 처리 장치에서 캐시가 업데이트되면 다른 처리 장치도 새로운 데이터로 자동 업데이트 되는 구조이다.
- 복제 캐시는 속도가 매우 빠르고 높은 수준의 내고장성을 지원하며 중앙 서버에 캐시를 갖고 있는 형태가 아니므로 단일 장애점이 없다
- 복제 캐시는 공간 기반 아키텍처의 표준 캐시 모델이지만 데이터량(캐시 크기)이 엄청나게 많거나 캐시 데이터가 너무 빈번하게 업데이트되는 등 복제 캐시를 사용할 수 없는 경우도 있다
  - 바로 이럴 때 분산 캐시를 사용하면 도움이 된다.

<img width="600" alt="스크린샷 2025-06-10 오후 5 25 02" src="https://github.com/user-attachments/assets/06baa2fa-81ac-46ec-99c8-d132086d9ab2" />

- 분산 캐시를 구현하려면 중아 캐시를 갖고 있는 전용 외부 서버 또는 서비스가 필요하다.
  - 처리 장치는 데이터를 내부 메모리에 저장하는 대신 전용 프로토콜을 통해 중앙 캐시 서버에 있는 데이터를 액세스한다.
  - 모든 데이터가 한 곳에 있고 복제할 필요가 없으니 분산 캐시는 높은 수준의 데이터 일관성을 보장한다
    - 캐시 데이터를 원격에서 가져와야 하므로 복제 캐시보다 성능이 낮고 시스템 전체 레이턴시가 증가한다
    - 데이터를 갖고 있는 캐시 서버가 다운되면 처리 장치에서 데이터를 액세스, 업데이트할 수 없기 때문에 모든 작동이 중단된다.

| 결정 기준 | 복제 캐시 | 분산 캐시 |
|----------|----------|----------|
| 최적화 | 성능 | 일관성 |
| 캐시 크기 | 작다(<100MB) | 크다(>500MB) |
| 데이터 유형 | 교적 정적임 | 매우 동적임 |
| 업데이트 빈도 | 비교적 낮음 | 매우 높음 |
| 네고장성 | 좋음 | 나쁨 |

- 캐시 크기가 비교적 작고(100MB 미만) 캐시 업데이트율이 낮은 편이라서 캐시 제품의 복제 엔진이 캐시 업데이트를 충분히 따라올 수 을 때 복제 캐시냐, 분산 캐시냐의 선택을 결국 데이터 일관성이냐, 성능/내고장성의 문제냐 이다.
  - 예를 들어 일관성이 중요한 데이터(예: 제품 재고)는 분산 캐시를 사용하고, 자주 변경되지 않는 데이터(예: 이름/값 쌍, 제품 코드, 제품 설명)는 복제 캐시를 사용한다.
- 공간 기반 아키텍처에서 캐시 모델을 선정할 때 대부분의 경우 주어진 애플리케이션 콘텍스트에서 두 모델 모두 적용 가능하다.
  - 복제 캐시든 분산 캐시든 어느 한 모델로 모든 문제를 해결할 수 없다.
 
# 15.5 니어 캐시

<img width="594" alt="스크린샷 2025-06-10 오후 5 32 25" src="https://github.com/user-attachments/assets/69df58e5-a4d0-4021-b4f6-659cafbca659" />

- 니어 캐시는 분산 캐시와 인메모리 데이터 그리드를 접합한 일종의 하이브리드 캐시 모델이다.
  - 분산 캐시는 풀 백킹 캐시, 각 처리 장치에 포함된 인메모리 데이터 그리드는 프런트 캐시라고 한다.
  - 프런트 캐시는 항상 풀 백킹 캐시보다 작은 서브셋을 담고 있고, 방출 정책을 통해 옛 항목을 삭제한 다음 최근 항목을 추가한다.
  - 프런트 캐시는 가장 최근에사용된 항목이 포함된(MRU) 캐시 또는 가장 자주 사용한 항목이 포함되는(MFU) 캐시로 사용한다.
- 프런트 캐시는 항상 풀 백킹 캐시와 동기화되지만 각 처리 장치에 포함된 프런트 캐시는 동일한 데이터를 공유하는 다른 처리 장치와 동기화 되지 않는다.
  - 동일한 데이터 콘텍스트(예: 고객 프로필)을 공유하는 여러 처리 장치가 동일하지 않은 데이터를 각자의 프런트 캐시에 소유하게 될 수도 있다.
  - 이처럼 처리 장치마다 상이한 데이터를 프런트 캐시에 갖게되고 처리 장치 간 성능과 응답성의 일관성이 결여된다. 그러므로 공간 기반 아키텍처에서 니어 캐시 모델은 권장되지 않는다.
 
# 15.6 구현 예시
- 공간 기반 아키텍처는 유저 수나 요청량이 갑자기 폭증하는 애플리케이션이나 10,000명이 넘는 동시 유저를 처리해야 하는 종류의 애플리케이션에 적합하다.
  - 온라인 콘서트 티켓 판매 시스템과 온라인 경매 시스템이 공간 기반 아키텍처로 구축하기에 좋은 후보들이다. 이런 시스템은 높은 확장성, 탄력성, 고성능이 보장되어야 하는 공통점이 있다.

### 15.6.1 콘서트 티켓 판매 시스템
- 평소에는 비교적 동시 유저 수가 적게 유지되다가 인기 있는 콘서트의 티켓 발매가 시작되면 티켓을 사려는 동시 유저 수가 수백~수천 단위로 급증하는 패턴을 보인다.
- 풀어야 할 난제가 한 두가지가 아니다.
  - 좌석 선호도와 상관없이 티켓 수량은 정해져 있다. 엄청난 동시 요청 수를 감당하려면 좌석 가능 여부가 가능한 한 신속하게, 지속적으로 업데이트 돼야 한다.
  - 좌석 지정제 콘서트라면 더 더욱 신속하게 업데이트가 되어야 한다
  - 따라서 시스템이 계속 중앙 데이터베이스에 동기적으로 액세스하면 머지않아 작동이 멎게 될 가능성이 매우 높다. 일반적인 데이터베이스가 이 정도로 표준 트랜잭션을 수만 개나 동시 처리하기는 매우 어렵다.
- 공간 기반 아키텍처는 이런 애플리케이션에서 꼭 필요한 고도의 탄력성을 제공하므로 안성 맞춤이다.
  - 동시 유저 수가 순간적으로 치솟으면 배포 관리자는 이 사실을 인지하여 다수의 처리 장치를 기동한다.

### 15.6.2 온라인 경매 시스템
- 온라인 콘서트 티켓 판매 시스템과 거의 똑같은 아키텍처 양상을 보인다. 고도의 성능과 탄력성이 필요하고, 유저 및 요청 부하가 예기치 않게 폭증하는 특징이 있다.
- 따라서 부하가 증가하면 처리 장치를 여럿 기동할 수 있는 공간 기반 아키텍처는 이런 종류의 문제 영역에 적합하다.
  - 경매가 끝나면 사용하지 않은 처리 장치는 삭제하면 되고, 각 처리 장치를 각 경매마다 할당하면 입찰 데이터의 일관성도 보장할 수 있다.
  - 데이터 펌프는 원래 비동기로 작동되므로 입찰 데이터를 긴 레이턴시 없이 다른 처리 장치로 보낼 수 있다.

# 15.7 아키텍처 특성 등급

| 아키텍처 특성 | 별점 |
|-------------|------|
| 분할 유형 | 도메인 + 기술 |
| 퀀텀 수 | 하나 또는 여러 개 |
| 배포성 | ⭐⭐⭐ |
| 탄력성 | ⭐⭐⭐⭐⭐ |
| 진화성 | ⭐⭐⭐ |
| 내고장성 | ⭐⭐⭐ |
| 모듈성 | ⭐⭐⭐ |
| 전체 비용 | ⭐⭐ |
| 성능 | ⭐⭐⭐⭐⭐ |
| 신뢰성 | ⭐⭐⭐⭐ |
| 확장성 | ⭐⭐⭐⭐⭐ |
| 단순성 | ⭐ |
| 시험성 | ⭐ |

- 탄력성, 확장성, 성능의 끝판왕이다. 이 아키텍처는 인메모리 데이터 캐시를 활용하고 제약조건에 해당되는 데이터베이스를 없앴기 때문에 이 세가지 특성을 높은 수준으로 달성할 수 있다.
- 전체적인 단순성과 시험성 측면에서는 트레이드오프가 있다.
  - 주요 데이터 저장소에 캐시를 사용하고 최종 일관성이라는 개념을 적용하기 때문에 구조가 매우 복잡한 아키텍처이다. 따라서 수많은 가동부에서 충돌이 발생해도 데이터가 소실되는 일이 없도록 주의를 기울여야 한다.
- 고도의 확장성과 탄력성을 시뮬레이션하는 복잡도 때문에 시험성은 낮다.
  - 최고 부하를 받는 상태에서 수십만명의 동시 유저를 테스트하는 일은 대단히 복잡하며 비용도 많이 든다.
- 비용 역시 고려해야 할 팩터이다.
  - 클라우드 및 온프레미스 시스템에서 높은 확장성과 탄력성을 실현하려면 상용 캐시 제품을 사용해야 하므로 상대적으로 돈이 많이 든다.
- 분할 유형은 식별하기가 쉽지 않아 `도메인 + 기술` 분할로 표시했다.
  - 특정 유형의 도메인에 따라 조정되고 처리 장치도 유연하므로 도메인 분할된다.
  - 처리 장치는 서비스 기반 아키텍처나 마이크로서비스 아키텍처에서 정의되는 것과 같이 일종의 도메인 서비스로 작동시킬 수 있다.
  - 동시에 데이터 펌프를 통해 데이터베이스의 실제 데이터 저장소에서 캐시를 이용해 트랜잭션 처리 문제를 분리하는 방식으로 기술 분할된다.
  - 처리 장치, 데이터 펌프, 데이터 리더/라이터, 데이터베이스는 모두 요청을 처리하는 기술 레이어를 구성한다. 모놀리식의 n-티어 구조와 아주 비슷하다.
- 퀀텀 수는 유저 인터페이스를 설계하는 방식과 처리 장치간 통신하는 방법에 따라 달라진다.
  - 처리 장치는 데이터베이스와 동기 통신을 하지 않기 때문에 데이터베이스 자체는 퀀텀 방정식의 일부로 포함되지 않는다.
  - 따라서 공간 기반 아키텍처의 퀀텀은 다양한 유저 인터페이스와 처리 장치가 연관된 형태로 나타난다. 상호 동기 통신(또는 오케스트레이션 하기 위해 처리 그리드를 거쳐 동기 통신)하는 처리 장치는 모두 동일한 아키텍처 퀀텀에 속한다.
